{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forager\n",
    "\n",
    "An agent that has competing drives - hide in a safe area or traverse risky territory to visit food, which resets hunger. Hunger increases at each timestep that it isn't visiting food."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inferactively-pymdp in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.0.7.1)\n",
      "Requirement already satisfied: attrs>=20.3.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (25.4.0)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.12.1)\n",
      "Requirement already satisfied: iniconfig>=1.1.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (2.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (1.4.9)\n",
      "Requirement already satisfied: matplotlib>=3.1.3 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (3.10.8)\n",
      "Requirement already satisfied: nose>=1.3.7 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (1.3.7)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (2.4.2)\n",
      "Requirement already satisfied: openpyxl>=3.0.7 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (3.1.5)\n",
      "Requirement already satisfied: packaging>=20.8 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from inferactively-pymdp) (25.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (3.0.0)\n",
      "Requirement already satisfied: Pillow>=8.2.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (12.0.0)\n",
      "Requirement already satisfied: pluggy>=0.13.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (1.6.0)\n",
      "Requirement already satisfied: py>=1.10.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (1.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (3.3.2)\n",
      "Requirement already satisfied: pytest>=6.2.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (9.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from inferactively-pymdp) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.5 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (1.17.0)\n",
      "Requirement already satisfied: seaborn>=0.11.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.13.2)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from inferactively-pymdp) (1.17.0)\n",
      "Requirement already satisfied: toml>=0.10.2 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (4.15.0)\n",
      "Requirement already satisfied: xlsxwriter>=1.4.3 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (3.2.9)\n",
      "Requirement already satisfied: sphinx-rtd-theme>=0.4 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (3.1.0)\n",
      "Requirement already satisfied: myst-nb>=0.13.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (1.3.0)\n",
      "Requirement already satisfied: autograd>=1.3 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (1.8.0)\n",
      "Requirement already satisfied: jax>=0.3.4 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.9.0)\n",
      "Requirement already satisfied: jaxlib>=0.3.4 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.9.0)\n",
      "Requirement already satisfied: equinox>=0.9 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.13.4)\n",
      "Requirement already satisfied: numpyro>=0.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.20.0)\n",
      "Requirement already satisfied: arviz>=0.13 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.23.3)\n",
      "Requirement already satisfied: optax>=0.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from inferactively-pymdp) (0.2.6)\n",
      "Requirement already satisfied: setuptools>=60.0.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from arviz>=0.13->inferactively-pymdp) (80.10.2)\n",
      "Requirement already satisfied: xarray>=2023.7.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from arviz>=0.13->inferactively-pymdp) (2026.1.0)\n",
      "Requirement already satisfied: h5netcdf>=1.0.2 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from arviz>=0.13->inferactively-pymdp) (1.8.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from arviz>=0.13->inferactively-pymdp) (3.15.1)\n",
      "Requirement already satisfied: xarray-einstats>=0.3 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from arviz>=0.13->inferactively-pymdp) (0.9.1)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from arviz>=0.13->inferactively-pymdp) (4.5.1)\n",
      "Requirement already satisfied: jaxtyping>=0.2.20 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from equinox>=0.9->inferactively-pymdp) (0.3.7)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from equinox>=0.9->inferactively-pymdp) (0.1.7)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jax>=0.3.4->inferactively-pymdp) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jax>=0.3.4->inferactively-pymdp) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.1.3->inferactively-pymdp) (1.3.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib>=3.1.3->inferactively-pymdp) (4.61.1)\n",
      "Requirement already satisfied: importlib_metadata in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (8.7.1)\n",
      "Requirement already satisfied: ipython in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (9.8.0)\n",
      "Requirement already satisfied: jupyter-cache>=0.5 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (1.0.1)\n",
      "Requirement already satisfied: nbclient in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (0.10.4)\n",
      "Requirement already satisfied: myst-parser>=1.0.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (5.0.0)\n",
      "Requirement already satisfied: nbformat>=5.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (5.10.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (6.0.3)\n",
      "Requirement already satisfied: sphinx>=5 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (9.1.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from myst-nb>=0.13.1->inferactively-pymdp) (7.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-cache>=0.5->myst-nb>=0.13.1->inferactively-pymdp) (8.3.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.3.12 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-cache>=0.5->myst-nb>=0.13.1->inferactively-pymdp) (2.0.46)\n",
      "Requirement already satisfied: tabulate in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-cache>=0.5->myst-nb>=0.13.1->inferactively-pymdp) (0.9.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy<3,>=1.3.12->jupyter-cache>=0.5->myst-nb>=0.13.1->inferactively-pymdp) (3.3.1)\n",
      "Requirement already satisfied: docutils<0.23,>=0.20 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-parser>=1.0.0->myst-nb>=0.13.1->inferactively-pymdp) (0.22.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-parser>=1.0.0->myst-nb>=0.13.1->inferactively-pymdp) (3.1.6)\n",
      "Requirement already satisfied: markdown-it-py~=4.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-parser>=1.0.0->myst-nb>=0.13.1->inferactively-pymdp) (4.0.0)\n",
      "Requirement already satisfied: mdit-py-plugins~=0.5 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from myst-parser>=1.0.0->myst-nb>=0.13.1->inferactively-pymdp) (0.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py~=4.0->myst-parser>=1.0.0->myst-nb>=0.13.1->inferactively-pymdp) (0.1.2)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.1.0)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.0.0)\n",
      "Requirement already satisfied: Pygments>=2.17 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.19.2)\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (3.0.1)\n",
      "Requirement already satisfied: babel>=2.13 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.18.0)\n",
      "Requirement already satisfied: alabaster>=0.7.14 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (1.0.0)\n",
      "Requirement already satisfied: imagesize>=1.3 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.30.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.32.5)\n",
      "Requirement already satisfied: roman-numerals>=1.0.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (4.1.0)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->myst-parser>=1.0.0->myst-nb>=0.13.1->inferactively-pymdp) (3.0.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from nbclient->myst-nb>=0.13.1->inferactively-pymdp) (8.7.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from nbclient->myst-nb>=0.13.1->inferactively-pymdp) (5.9.1)\n",
      "Requirement already satisfied: traitlets>=5.4 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from nbclient->myst-nb>=0.13.1->inferactively-pymdp) (5.14.3)\n",
      "Requirement already satisfied: pyzmq>=25.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-client>=6.1.12->nbclient->myst-nb>=0.13.1->inferactively-pymdp) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.4.1 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from jupyter-client>=6.1.12->nbclient->myst-nb>=0.13.1->inferactively-pymdp) (6.5.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbformat>=5.0->myst-nb>=0.13.1->inferactively-pymdp) (2.21.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbformat>=5.0->myst-nb>=0.13.1->inferactively-pymdp) (4.26.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.0->myst-nb>=0.13.1->inferactively-pymdp) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.0->myst-nb>=0.13.1->inferactively-pymdp) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.0->myst-nb>=0.13.1->inferactively-pymdp) (0.30.0)\n",
      "Requirement already satisfied: multipledispatch in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from numpyro>=0.1->inferactively-pymdp) (1.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from numpyro>=0.1->inferactively-pymdp) (4.67.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openpyxl>=3.0.7->inferactively-pymdp) (2.0.0)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optax>=0.1->inferactively-pymdp) (2.4.0)\n",
      "Requirement already satisfied: chex>=0.1.87 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optax>=0.1->inferactively-pymdp) (0.1.91)\n",
      "Requirement already satisfied: toolz>=1.0.0 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from chex>=0.1.87->optax>=0.1->inferactively-pymdp) (1.1.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2.4->inferactively-pymdp) (2025.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.30.0->sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.30.0->sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.30.0->sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.30.0->sphinx>=5->myst-nb>=0.13.1->inferactively-pymdp) (2026.1.4)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sphinx-rtd-theme>=0.4->inferactively-pymdp) (4.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\ryan\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from importlib_metadata->myst-nb>=0.13.1->inferactively-pymdp) (3.23.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->myst-nb>=0.13.1->inferactively-pymdp) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->myst-nb>=0.13.1->inferactively-pymdp) (1.8.19)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->myst-nb>=0.13.1->inferactively-pymdp) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->myst-nb>=0.13.1->inferactively-pymdp) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipykernel->myst-nb>=0.13.1->inferactively-pymdp) (7.2.1)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipython->myst-nb>=0.13.1->inferactively-pymdp) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipython->myst-nb>=0.13.1->inferactively-pymdp) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipython->myst-nb>=0.13.1->inferactively-pymdp) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipython->myst-nb>=0.13.1->inferactively-pymdp) (3.0.52)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from ipython->myst-nb>=0.13.1->inferactively-pymdp) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->myst-nb>=0.13.1->inferactively-pymdp) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from jedi>=0.18.1->ipython->myst-nb>=0.13.1->inferactively-pymdp) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from stack_data>=0.6.0->ipython->myst-nb>=0.13.1->inferactively-pymdp) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from stack_data>=0.6.0->ipython->myst-nb>=0.13.1->inferactively-pymdp) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ryan\\appdata\\roaming\\python\\python313\\site-packages (from stack_data>=0.6.0->ipython->myst-nb>=0.13.1->inferactively-pymdp) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install inferactively-pymdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_TKG9Dr5tuT"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_LjE_6_5tuT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "from pymdp.agent import Agent\n",
    "from pymdp import utils, maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4544465a"
   },
   "source": [
    "# Task\n",
    "Develop and analyze an active inference agent for a camping and foraging task within a custom grid world environment. This involves:\n",
    "\n",
    "1.  **Environment Setup**: Defining a `4x4` grid with 'food' at `(0,3)` and 'shelter' at `(0,0)`, a maximum hunger level of `5`, and a maximum risk level of `5`.\n",
    "2.  **Generative Model Design** (Using **Factorized State Space** with `B_factor_list`):\n",
    "    *   **Hidden State Factors**: `Location` (16 grid points), `Hunger Level` (0-5), and `Risk Level` (0-5) as **separate factors**.\n",
    "    *   **Observations**: `Location Observation`, `Hunger Sensor`, and `Risk Sensor`.\n",
    "    *   **`A` Matrix**: Multi-dimensional arrays mapping states to observations, with location-dependent perception (satiated at food, safe at shelter).\n",
    "    *   **`B` Matrix** with **`B_factor_list`**: \n",
    "        - `B[0]` (Location): Standard 5-action movement, depends only on Location.\n",
    "        - `B[1]` (Hunger): **Location-dependent** transitions via `B_factor_list=[[0], [0,1], [0,2]]` - resets at food, otherwise increases.\n",
    "        - `B[2]` (Risk): **Location-dependent** transitions - resets at shelter, otherwise increases.\n",
    "    *   **`C` Vector**: Preferences for low hunger and low risk observations.\n",
    "    *   **`D` Vector**: Initial belief of starting location with hunger 0 and risk 0.\n",
    "3.  **Generative Process (`GridWorldEnv`)**: Implementing a custom class that simulates true environment dynamics.\n",
    "4.  **Simulation & Analysis**: Running a simulation, tracking the agent's path, hunger, and risk, visualizing the results.\n",
    "\n",
    "**Key Advantage**: Factorized state space with `B_factor_list` scales as O(n) instead of O(n³), making larger grids and longer policy horizons tractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dfe8091"
   },
   "source": [
    "## Define Environment Parameters\n",
    "\n",
    "### Subtask:\n",
    "Establish the grid dimensions, specific locations for 'food' and 'shelter' squares, a maximum hunger level, and how risk factors will be assigned to non-shelter squares. Also define the mapping of linear indices to (y,x) coordinates for the grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3c92f12"
   },
   "source": [
    "**Reasoning**:\n",
    "The user provided specific instructions for defining the environment parameters, including grid dimensions, food/shelter locations, max hunger, and risk factors. I will generate the Python code to set up these parameters as specified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec23236b",
    "outputId": "6adeee19-fee1-49cc-cf69-fcad0e8cdef8"
   },
   "outputs": [],
   "source": [
    "# Scalable grid world configuration\n",
    "grid_dims = [5, 5]  # 5x5 grid (25 locations)\n",
    "max_hunger = 7      # 0-7 (8 levels)\n",
    "max_risk = 7        # 0-7 (8 levels)\n",
    "\n",
    "# Food and shelter are 8 steps apart along the diagonal\n",
    "shelter_location = (0, 0) # Top-left corner\n",
    "food_location = (4, 0)    # Bottom-left corner\n",
    "\n",
    "num_grid_points = int(np.prod(grid_dims))  # 25 grid locations\n",
    "\n",
    "# Create a look-up table `loc_list` that maps linear indices to (y, x) coordinates\n",
    "grid = np.arange(num_grid_points).reshape(grid_dims)\n",
    "it = np.nditer(grid, flags=[\"multi_index\"])\n",
    "\n",
    "loc_list = []\n",
    "while not it.finished:\n",
    "    loc_list.append(it.multi_index)\n",
    "    it.iternext()\n",
    "\n",
    "distance = abs(food_location[0]-shelter_location[0]) + abs(food_location[1]-shelter_location[1])\n",
    "\n",
    "print(f\"Grid Dimensions: {grid_dims}\")\n",
    "print(f\"Maximum Hunger/Risk Level: {max_hunger}\")\n",
    "print(f\"Food Location: {food_location}\")\n",
    "print(f\"Shelter Location: {shelter_location}\")\n",
    "print(f\"Manhattan distance: {distance} steps\")\n",
    "print(f\"\\nFactorized state space:\")\n",
    "print(f\"  {num_grid_points} locations + {max_hunger+1} hunger + {max_risk+1} risk = {num_grid_points + max_hunger+1 + max_risk+1} parameters\")\n",
    "print(f\"  (vs unified: {num_grid_points * (max_hunger+1) * (max_risk+1)} states)\")\n",
    "print(f\"\\n*** Journey ({distance} steps) < Max level ({max_hunger}) = Agent survives! ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2a10adc"
   },
   "source": [
    "## Define Hidden State Factors\n",
    "\n",
    "### Subtask:\n",
    "Specify the agent's hidden state factors (`num_states`). You will need three factors: 1) Location (the agent's position in the grid), 2) Hunger Level (discrete levels from 0 to `max_hunger`), and 3) Risk Level (discrete levels from 0 to `max_risk`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e58f636"
   },
   "source": [
    "#### Instructions\n",
    "1. Define `hunger_levels` and `risk_levels` as NumPy arrays ranging from 0 to their respective max values.\n",
    "2. Create `num_states = [num_grid_points, num_hunger, num_risk]` as **three separate factors** (not unified).\n",
    "3. This factorized representation scales linearly O(n) instead of O(n³) with the unified approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "091d400d"
   },
   "source": [
    "**Reasoning**:\n",
    "Using a **factorized state space** with `B_factor_list` instead of a unified state space. This keeps Location, Hunger, and Risk as separate factors while still allowing location-dependent dynamics through the transition model dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc25d822",
    "outputId": "059e71d9-7869-4be3-eb90-12b1fcb4f22e"
   },
   "outputs": [],
   "source": [
    "hunger_levels = np.arange(max_hunger + 1)  # 0 to max_hunger, inclusive\n",
    "risk_levels = np.arange(max_risk + 1)  # 0 to max_risk, inclusive\n",
    "\n",
    "# Pre-compute indices for special locations\n",
    "food_idx = loc_list.index(food_location)\n",
    "shelter_idx = loc_list.index(shelter_location)\n",
    "\n",
    "# FACTORIZED STATE SPACE: Keep Location, Hunger, Risk as separate factors\n",
    "# Use B_factor_list to encode location-dependent transitions for hunger/risk\n",
    "num_hunger = len(hunger_levels)\n",
    "num_risk = len(risk_levels)\n",
    "\n",
    "# Hidden state factors: [Location, Hunger, Risk]\n",
    "num_states = [num_grid_points, num_hunger, num_risk]\n",
    "\n",
    "print(f\"Factorized State Space:\")\n",
    "print(f\"  Factor 0 (Location): {num_grid_points} states\")\n",
    "print(f\"  Factor 1 (Hunger): {num_hunger} states (0-{max_hunger})\")\n",
    "print(f\"  Factor 2 (Risk): {num_risk} states (0-{max_risk})\")\n",
    "print(f\"  Total: {num_grid_points} + {num_hunger} + {num_risk} = {sum(num_states)} parameters\")\n",
    "print(f\"  (vs unified: {num_grid_points * num_hunger * num_risk} states)\")\n",
    "print(f\"\\nFood at index {food_idx} {food_location}, Shelter at index {shelter_idx} {shelter_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61d4bbaf"
   },
   "source": [
    "## Define Observation Modalities\n",
    "\n",
    "### Subtask:\n",
    "Specify the agent's observation modalities (`num_obs`). You will need three modalities: 1) Location Observation (the agent observes its position), 2) Hunger Sensor (the agent observes its internal hunger state), and 3) Risk Sensor (the agent observes its internal risk state)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60b5c131"
   },
   "source": [
    "**Reasoning**:\n",
    "I will define the names for each observation modality and then create the `num_obs` list based on their dimensionalities, as specified in the instructions. This will complete the subtask of specifying the observation modalities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e52b21c",
    "outputId": "7ec061f8-3f9b-41f7-d665-cf985b7dd996"
   },
   "outputs": [],
   "source": [
    "location_obs_names = [f\"({y},{x})\" for y,x in loc_list] # For Location Observation\n",
    "hunger_sensor_names = ['satiated'] + [f'hunger_{h}' for h in range(1, max_hunger + 1)] # 'satiated' is hunger_0\n",
    "risk_sensor_names = ['safe'] + [f'risk_{r}' for r in range(1, max_risk + 1)] # 'safe' is risk_0\n",
    "\n",
    "# Observation modalities and their levels\n",
    "num_obs = [\n",
    "    num_grid_points, # Location Observation (number of grid points)\n",
    "    len(hunger_sensor_names), # Hunger Sensor (satiated, hunger_1, ..., hunger_max_hunger)\n",
    "    len(risk_sensor_names) # Risk Sensor (safe, risk_1, ..., risk_max_risk)\n",
    "]\n",
    "\n",
    "print(f\"Observation Dimensionalities (Location, Hunger Sensor, Risk Sensor): {num_obs}\")\n",
    "print(f\"Hunger Sensor Names: {hunger_sensor_names}\")\n",
    "print(f\"Risk Sensor Names: {risk_sensor_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d8e603b"
   },
   "source": [
    "## Design Observation Model (A matrix)\n",
    "\n",
    "### Subtask:\n",
    "Construct the `A` matrix, paying close attention to dependencies for each observation modality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a224e86"
   },
   "source": [
    "**Reasoning**:\n",
    "For the factorized state space, each A matrix has shape `(num_obs[m], num_states[0], num_states[1], num_states[2])`, i.e., observation dimension × all state factor dimensions. This allows observations to depend on any combination of hidden state factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94a05c3b",
    "outputId": "93eb7919-b830-4f01-de7a-4817967f1f0a"
   },
   "outputs": [],
   "source": [
    "# A matrix shapes for factorized state space:\n",
    "# Each A[m] has shape (num_obs[m], num_states[0], num_states[1], num_states[2])\n",
    "# i.e., (num_obs[m], num_locations, num_hunger, num_risk)\n",
    "\n",
    "A_m_shapes = [[num_obs[m]] + num_states for m in range(len(num_obs))]\n",
    "A = utils.obj_array_zeros(A_m_shapes)\n",
    "\n",
    "print(f\"Building A matrices for factorized state space...\")\n",
    "print(f\"  A[0] Location obs shape: {A[0].shape}\")\n",
    "print(f\"  A[1] Hunger sensor shape: {A[1].shape}\")\n",
    "print(f\"  A[2] Risk sensor shape: {A[2].shape}\")\n",
    "\n",
    "# Fill A matrices\n",
    "for loc_idx in range(num_grid_points):\n",
    "    for h in range(num_hunger):\n",
    "        for r in range(num_risk):\n",
    "            # A[0] - Location Observation: identity mapping for location\n",
    "            A[0][loc_idx, loc_idx, h, r] = 1.0\n",
    "            \n",
    "            # A[1] - Hunger Sensor\n",
    "            # At food: always observe 'satiated' (index 0)\n",
    "            # Elsewhere: observe true hunger level\n",
    "            if loc_idx == food_idx:\n",
    "                A[1][0, loc_idx, h, r] = 1.0  # Always satiated at food\n",
    "            else:\n",
    "                A[1][h, loc_idx, h, r] = 1.0  # True hunger elsewhere\n",
    "            \n",
    "            # A[2] - Risk Sensor\n",
    "            # At shelter: always observe 'safe' (index 0)\n",
    "            # Elsewhere: observe true risk level\n",
    "            if loc_idx == shelter_idx:\n",
    "                A[2][0, loc_idx, h, r] = 1.0  # Always safe at shelter\n",
    "            else:\n",
    "                A[2][r, loc_idx, h, r] = 1.0  # True risk elsewhere\n",
    "\n",
    "print(\"\\nAll A matrices filled.\")\n",
    "print(\"  Food location -> always 'satiated' observation\")\n",
    "print(\"  Shelter location -> always 'safe' observation\")\n",
    "\n",
    "# Verification: check normalization\n",
    "all_normalized = True\n",
    "for m in range(len(A)):\n",
    "    # Sum over observation dimension (axis 0) should be 1 for all state configs\n",
    "    sum_over_obs = A[m].sum(axis=0)\n",
    "    is_norm_m = np.allclose(sum_over_obs, 1.0)\n",
    "    if not is_norm_m:\n",
    "        print(f\"Warning: A[{m}] is NOT normalized!\")\n",
    "        all_normalized = False\n",
    "print(f\"All A sub-arrays are normalized: {all_normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c12f0cb"
   },
   "source": [
    "## Design Transition Model (B matrix)\n",
    "\n",
    "### Subtask:\n",
    "Construct the `B` matrix using **`B_factor_list`** to encode location-dependent dynamics while keeping the state space factorized.\n",
    "\n",
    "#### Key Concept: B_factor_list\n",
    "The `B_factor_list` parameter specifies which hidden state factors each transition matrix depends on:\n",
    "- `B_factor_list = [[0], [0, 1], [0, 2]]` means:\n",
    "  - `B[0]` (Location) depends only on Location (factor 0) - standard movement\n",
    "  - `B[1]` (Hunger) depends on Location AND Hunger - **location-dependent transitions!**\n",
    "  - `B[2]` (Risk) depends on Location AND Risk - **location-dependent transitions!**\n",
    "\n",
    "This allows encoding that:\n",
    "- Hunger resets to 0 when at the food location\n",
    "- Risk resets to 0 when at the shelter location\n",
    "\n",
    "...while keeping the state space factorized (scalable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eea17df2"
   },
   "source": [
    "**Reasoning**:\n",
    "With the factorized approach:\n",
    "- `num_controls = [5, 1, 1]` - Location has 5 movement actions, Hunger and Risk have 1 null action each (uncontrollable)\n",
    "- `B_factor_list = [[0], [0, 1], [0, 2]]` - specifies dependencies for location-dependent dynamics\n",
    "- `control_fac_idx = [0]` - only Location factor is under agent's control\n",
    "\n",
    "B matrix shapes follow the pattern: `(next_state, *parent_factor_dims, num_actions)`\n",
    "- For B_factor_list[1] = [0, 1]: B[1] has shape `(6, 16, 6, 1)` = `(next_hunger, location, curr_hunger, 1 action)`\n",
    "- For B_factor_list[2] = [0, 2]: B[2] has shape `(6, 16, 6, 1)` = `(next_risk, location, curr_risk, 1 action)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67dad7e0",
    "outputId": "c63e45e6-5768-441e-ee15-4d236de6fdbe"
   },
   "outputs": [],
   "source": [
    "# FACTORIZED TRANSITION MODEL\n",
    "# num_controls: actions per factor - only Location (factor 0) is controllable\n",
    "num_controls = [5, 1, 1]  # 5 movement actions, 1 null action for hunger, 1 null action for risk\n",
    "\n",
    "# B_factor_list specifies which state factors each B matrix depends on:\n",
    "# - B[0] (Location): depends only on Location (factor 0) - standard\n",
    "# - B[1] (Hunger): depends on Location (factor 0) AND Hunger (factor 1) - location-dependent!\n",
    "# - B[2] (Risk): depends on Location (factor 0) AND Risk (factor 2) - location-dependent!\n",
    "B_factor_list = [[0], [0, 1], [0, 2]]\n",
    "\n",
    "# Specify only Location (factor 0) is controllable by agent\n",
    "control_fac_idx = [0]\n",
    "\n",
    "# Initialize B matrices with appropriate shapes\n",
    "# Shape pattern: (next_state, *parent_factor_dims, num_actions)\n",
    "# For B_factor_list = [[0], [0, 1], [0, 2]]:\n",
    "# B[0]: (16, 16, 5) - next_loc, curr_loc, 5 actions\n",
    "# B[1]: (6, 16, 6, 1) - next_hunger, curr_loc (factor 0), curr_hunger (factor 1), 1 null action\n",
    "# B[2]: (6, 16, 6, 1) - next_risk, curr_loc (factor 0), curr_risk (factor 2), 1 null action\n",
    "\n",
    "B = utils.obj_array(len(num_states))\n",
    "B[0] = np.zeros((num_grid_points, num_grid_points, num_controls[0]))  # (16, 16, 5)\n",
    "B[1] = np.zeros((num_hunger, num_grid_points, num_hunger, num_controls[1]))  # (6, 16, 6, 1)\n",
    "B[2] = np.zeros((num_risk, num_grid_points, num_risk, num_controls[2]))  # (6, 16, 6, 1)\n",
    "\n",
    "print(\"B matrices initialized for factorized state space:\")\n",
    "print(f\"  B[0] Location: {B[0].shape} - depends on [Location]\")\n",
    "print(f\"  B[1] Hunger: {B[1].shape} - depends on [Location, Hunger]\")\n",
    "print(f\"  B[2] Risk: {B[2].shape} - depends on [Location, Risk]\")\n",
    "print(f\"\\nB_factor_list: {B_factor_list}\")\n",
    "print(f\"control_fac_idx: {control_fac_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6439d56"
   },
   "source": [
    "#### Instructions\n",
    "1. Fill out `B[0]` (Location transitions):\n",
    "    * Iterate through each of the 5 possible actions (UP, DOWN, LEFT, RIGHT, STAY).\n",
    "    * For each action and current location, calculate the next location based on grid boundaries.\n",
    "    * Set `B[0][next_loc, curr_loc, action_id] = 1.0` for deterministic transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bc8cbc6"
   },
   "source": [
    "**Reasoning**:\n",
    "Filling `B[0]` with standard grid movement transitions. This factor only depends on itself (not on hunger or risk), which is reflected in `B_factor_list[0] = [0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e318dc05",
    "outputId": "28528d4b-badc-475f-9e81-167de5baead7"
   },
   "outputs": [],
   "source": [
    "actions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"STAY\"]\n",
    "\n",
    "print(\"Filling B[0] - Location transitions...\")\n",
    "\n",
    "# B[0]: Location factor transitions based on movement actions\n",
    "for action_id, action_label in enumerate(actions):\n",
    "    for curr_loc_idx in range(num_grid_points):\n",
    "        curr_y, curr_x = loc_list[curr_loc_idx]\n",
    "        \n",
    "        # Compute next location based on action\n",
    "        if action_label == \"UP\":\n",
    "            next_y = curr_y - 1 if curr_y > 0 else curr_y\n",
    "            next_x = curr_x\n",
    "        elif action_label == \"DOWN\":\n",
    "            next_y = curr_y + 1 if curr_y < (grid_dims[0]-1) else curr_y\n",
    "            next_x = curr_x\n",
    "        elif action_label == \"LEFT\":\n",
    "            next_y = curr_y\n",
    "            next_x = curr_x - 1 if curr_x > 0 else curr_x\n",
    "        elif action_label == \"RIGHT\":\n",
    "            next_y = curr_y\n",
    "            next_x = curr_x + 1 if curr_x < (grid_dims[1]-1) else curr_x\n",
    "        elif action_label == \"STAY\":\n",
    "            next_y, next_x = curr_y, curr_x\n",
    "        \n",
    "        next_loc = (next_y, next_x)\n",
    "        next_loc_idx = loc_list.index(next_loc)\n",
    "        \n",
    "        # Set transition probability\n",
    "        B[0][next_loc_idx, curr_loc_idx, action_id] = 1.0\n",
    "\n",
    "print(\"B[0] Location transitions complete!\")\n",
    "print(f\"  Actions: {actions}\")\n",
    "\n",
    "# Verify normalization\n",
    "for action_id in range(5):\n",
    "    col_sums = B[0][:, :, action_id].sum(axis=0)\n",
    "    if not np.allclose(col_sums, 1.0):\n",
    "        print(f\"  Warning: B[0][:,:,{action_id}] not normalized!\")\n",
    "print(\"  B[0] verified: all columns sum to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10678f67"
   },
   "source": [
    "#### Instructions\n",
    "1. Fill out `B[1]` (Hunger transitions) - **Location-dependent**:\n",
    "    * Shape: `(num_hunger, num_hunger, num_locations, 1)` - depends on both location and hunger.\n",
    "    * At food location: hunger resets to 0 regardless of current hunger.\n",
    "    * Elsewhere: hunger increases by 1, capped at `max_hunger`.\n",
    "2. Fill out `B[2]` (Risk transitions) - **Location-dependent**:\n",
    "    * Shape: `(num_risk, num_risk, num_locations, 1)` - depends on both location and risk.\n",
    "    * At shelter location: risk resets to 0 regardless of current risk.\n",
    "    * Elsewhere: risk increases by 1, capped at `max_risk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "768f1a12"
   },
   "source": [
    "**Reasoning**:\n",
    "This is where `B_factor_list` enables the key behavior:\n",
    "- `B[1]` has shape that includes the location dimension, allowing hunger to reset specifically at the food location.\n",
    "- `B[2]` similarly depends on location, allowing risk to reset at the shelter.\n",
    "This achieves location-dependent dynamics while keeping the state space factorized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60238d9d",
    "outputId": "08f2bfcc-35ca-49e2-844c-9c0502d21bd3"
   },
   "outputs": [],
   "source": [
    "# B[1] - Hunger transitions: LOCATION-DEPENDENT!\n",
    "# Shape: (num_hunger, num_locations, num_hunger, 1 null action)\n",
    "# Indexed as: B[1][next_hunger, location, curr_hunger, action]\n",
    "# - At food location: hunger resets to 0\n",
    "# - Elsewhere: hunger increases by 1 (capped at max)\n",
    "\n",
    "print(\"Filling B[1] - Hunger transitions (location-dependent)...\")\n",
    "\n",
    "for loc_idx in range(num_grid_points):\n",
    "    for curr_hunger in range(num_hunger):\n",
    "        if loc_idx == food_idx:\n",
    "            # AT FOOD: hunger resets to 0 regardless of current hunger\n",
    "            next_hunger = 0\n",
    "        else:\n",
    "            # ELSEWHERE: hunger increases by 1, capped at max\n",
    "            next_hunger = min(curr_hunger + 1, max_hunger)\n",
    "        \n",
    "        # Single null action (action_id=0)\n",
    "        B[1][next_hunger, loc_idx, curr_hunger, 0] = 1.0\n",
    "\n",
    "print(f\"  At food (loc {food_idx}): hunger -> 0\")\n",
    "print(f\"  Elsewhere: hunger -> min(hunger+1, {max_hunger})\")\n",
    "\n",
    "# B[2] - Risk transitions: LOCATION-DEPENDENT!\n",
    "# Shape: (num_risk, num_locations, num_risk, 1 null action)\n",
    "# Indexed as: B[2][next_risk, location, curr_risk, action]\n",
    "# - At shelter location: risk resets to 0\n",
    "# - Elsewhere: risk increases by 1 (capped at max)\n",
    "\n",
    "print(\"\\nFilling B[2] - Risk transitions (location-dependent)...\")\n",
    "\n",
    "for loc_idx in range(num_grid_points):\n",
    "    for curr_risk in range(num_risk):\n",
    "        if loc_idx == shelter_idx:\n",
    "            # AT SHELTER: risk resets to 0 regardless of current risk\n",
    "            next_risk = 0\n",
    "        else:\n",
    "            # ELSEWHERE: risk increases by 1, capped at max\n",
    "            next_risk = min(curr_risk + 1, max_risk)\n",
    "        \n",
    "        # Single null action (action_id=0)\n",
    "        B[2][next_risk, loc_idx, curr_risk, 0] = 1.0\n",
    "\n",
    "print(f\"  At shelter (loc {shelter_idx}): risk -> 0\")\n",
    "print(f\"  Elsewhere: risk -> min(risk+1, {max_risk})\")\n",
    "\n",
    "# Verify all B matrices\n",
    "print(\"\\nVerifying B matrix normalization...\")\n",
    "for f, factor_name in enumerate(['Location', 'Hunger', 'Risk']):\n",
    "    # Check that columns sum to 1 for all control/parent configurations\n",
    "    B_f = B[f]\n",
    "    # Sum over first axis (next state)\n",
    "    total_axes = tuple(range(1, len(B_f.shape)))  # All axes except the first\n",
    "    for idx in np.ndindex(B_f.shape[1:]):\n",
    "        col_sum = B_f[(slice(None),) + idx].sum()\n",
    "        if not np.isclose(col_sum, 1.0):\n",
    "            print(f\"  Warning: B[{f}] column at {idx} sums to {col_sum:.3f}\")\n",
    "print(\"All B matrices verified!\")\n",
    "\n",
    "print(\"\\n=== FACTORIZED DYNAMICS SUMMARY ===\")\n",
    "print(f\"Location transitions: standard grid movement (5 actions)\")\n",
    "print(f\"Hunger transitions: +1 per step, reset to 0 at food {food_location}\")\n",
    "print(f\"Risk transitions: +1 per step, reset to 0 at shelter {shelter_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15f26c50"
   },
   "source": [
    "## Prior Preferences (C vector)\n",
    "\n",
    "### Subtask:\n",
    "Define the agent's prior preferences (`C` vector). This involves setting high positive preferences for observations associated with well-being (e.g., low hunger, low risk) and high negative preferences for observations associated with undesirable states (e.g., high hunger, high risk).\n",
    "\n",
    "#### Instructions\n",
    "1. Initialize the `C` vector as an object array with all zeros, using `utils.obj_array_zeros` and `num_obs`.\n",
    "2. Set preferences for the `Hunger Sensor` observation modality (`C[1]`):\n",
    "    * Assign a high positive value to `'satiated'` (hunger 0).\n",
    "    * Assign moderate positive values to low hunger levels.\n",
    "    * Assign negative values to moderate hunger levels.\n",
    "    * Assign a high negative value to `max_hunger`.\n",
    "3. Set preferences for the `Risk Sensor` observation modality (`C[3]`):\n",
    "    * Use the same preference distribution as hunger - positive for 'safe', increasingly negative for higher risk levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c02aab2"
   },
   "source": [
    "**Reasoning**:\n",
    "I will now initialize the `C` vector and set the preferences for the `Hunger Sensor` and `Risk Feedback` observation modalities, as per the instructions in the previous markdown block.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef0a471e",
    "outputId": "27c12864-556a-481b-ebec-f324f4083ccf"
   },
   "outputs": [],
   "source": [
    "C = utils.obj_array_zeros(num_obs)\n",
    "\n",
    "# Define key preference values\n",
    "reward_satiated = 10.0\n",
    "penalty_max = -50.0\n",
    "penalty_approach = -8.0\n",
    "\n",
    "# Hunger preferences\n",
    "# Scale preferences automatically based on number of hunger levels\n",
    "n_hunger = len(C[1])\n",
    "\n",
    "C[1][0] = reward_satiated    # satiated - good reward\n",
    "C[1][-1] = penalty_max       # max hunger - CATASTROPHIC!\n",
    "\n",
    "# Intermediate levels: Quadratic scaling from 0 to penalty_approach\n",
    "# Map indices 1 to n_hunger-2 to 0 to -8.0\n",
    "if n_hunger > 2:\n",
    "    for i in range(1, n_hunger - 1):\n",
    "        # Normalize i from 0 (at i=1) to 1 (at i=n_hunger-2)\n",
    "        denominator = (n_hunger - 2) - 1\n",
    "        progress = (i - 1) / denominator if denominator > 0 else 1.0\n",
    "        \n",
    "        C[1][i] = penalty_approach * (progress ** 2)\n",
    "\n",
    "# Risk preferences - symmetric with hunger\n",
    "n_risk = len(C[2])\n",
    "\n",
    "C[2][0] = reward_satiated    # safe - good reward\n",
    "C[2][-1] = penalty_max       # max risk - CATASTROPHIC!\n",
    "\n",
    "if n_risk > 2:\n",
    "    for i in range(1, n_risk - 1):\n",
    "        denominator = (n_risk - 2) - 1\n",
    "        progress = (i - 1) / denominator if denominator > 0 else 1.0\n",
    "        \n",
    "        C[2][i] = penalty_approach * (progress ** 2)\n",
    "\n",
    "print(\"C (prior preferences) vector filled.\")\n",
    "print(f\"Hunger preferences: {np.round(C[1], 2)}\")\n",
    "print(f\"Risk preferences: {np.round(C[2], 2)}\")\n",
    "print(\"\\nPreferences scaled automatically to state space dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07eb5cad"
   },
   "source": [
    "## Design Prior Over Initial States (D vector)\n",
    "\n",
    "### Subtask:\n",
    "Specify the agent's prior over initial hidden states, the `D` array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b114a5e"
   },
   "source": [
    "#### Instructions\n",
    "1. Initialize `D` with uniform priors for each of the 3 factors using `utils.obj_array_uniform(num_states)`.\n",
    "2. Set `D[0]` - Location: one-hot belief for starting position.\n",
    "3. Set `D[1]` - Hunger: one-hot belief for initial hunger level (0 = satiated).\n",
    "4. Set `D[2]` - Risk: one-hot belief for initial risk level (0 = safe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d6c6158"
   },
   "source": [
    "**Reasoning**:\n",
    "With factorized states, `D` has 3 separate priors - one for each factor. Each is a one-hot vector specifying the agent's belief about its initial state for that factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fc1d013",
    "outputId": "1bb5ab71-e294-48c0-e1a3-18c0b528841e"
   },
   "outputs": [],
   "source": [
    "# D vector for factorized state space - one prior per factor\n",
    "D = utils.obj_array_uniform(num_states)\n",
    "\n",
    "# Set agent's initial beliefs:\n",
    "# - Location: center of grid (2,2)\n",
    "# - Hunger: 0 (satiated)\n",
    "# - Risk: 0 (safe)\n",
    "start_loc = (2, 2)\n",
    "start_loc_idx = loc_list.index(start_loc)\n",
    "start_hunger = 0\n",
    "start_risk = 0\n",
    "\n",
    "# One-hot vectors for each factor\n",
    "D[0] = utils.onehot(start_loc_idx, num_grid_points)  # Location\n",
    "D[1] = utils.onehot(start_hunger, num_hunger)        # Hunger\n",
    "D[2] = utils.onehot(start_risk, num_risk)            # Risk\n",
    "\n",
    "print(\"D (prior over initial hidden states) vector filled.\")\n",
    "print(f\"  D[0] Location: starts at {start_loc} (idx {start_loc_idx})\")\n",
    "print(f\"  D[1] Hunger: starts at {start_hunger}\")\n",
    "print(f\"  D[2] Risk: starts at {start_risk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09b7ef22"
   },
   "source": [
    "## Implement Generative Process (Environment)\n",
    "\n",
    "### Subtask:\n",
    "Create a custom `GridWorldEnv` class. This class will handle the true dynamics of the environment: tracking the agent's actual location, implementing hunger increment/reset rules, generating risk feedback based on randomly assigned risk factors for non-shelter squares, and returning the correct multi-modal observations to the agent based on its actions and the environment's state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0a070f7"
   },
   "source": [
    "**Reasoning**:\n",
    "I will define the `GridWorldEnv` class with `__init__`, `step`, and `reset` methods as specified in the instructions, implementing the environment's true dynamics for location, hunger, and observation generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e89679b4"
   },
   "outputs": [],
   "source": [
    "class GridWorldEnv():\n",
    "\n",
    "    def __init__(self, starting_loc, initial_hunger, initial_risk, food_location, shelter_location, \n",
    "                 grid_dims, max_hunger, max_risk, loc_list, hunger_levels, risk_levels,\n",
    "                 hunger_sensor_names, risk_sensor_names):\n",
    "        self.init_loc = starting_loc\n",
    "        self.init_hunger = initial_hunger\n",
    "        self.init_risk = initial_risk\n",
    "        self.current_location = self.init_loc\n",
    "        self.current_hunger = self.init_hunger\n",
    "        self.current_risk = self.init_risk\n",
    "\n",
    "        self.food_location = food_location\n",
    "        self.shelter_location = shelter_location\n",
    "        self.grid_dims = grid_dims\n",
    "        self.max_hunger = max_hunger\n",
    "        self.max_risk = max_risk\n",
    "        self.loc_list = loc_list\n",
    "        self.hunger_levels = hunger_levels\n",
    "        self.risk_levels = risk_levels\n",
    "        self.hunger_sensor_names = hunger_sensor_names\n",
    "        self.risk_sensor_names = risk_sensor_names\n",
    "\n",
    "        print(f'Environment Initialized: Starting location is {self.init_loc}, Initial hunger is {self.init_hunger}, Initial risk is {self.init_risk}')\n",
    "\n",
    "    def step(self, action_label):\n",
    "\n",
    "        Y, X = self.current_location\n",
    "\n",
    "        if action_label == \"UP\":\n",
    "          Y_new = Y - 1 if Y > 0 else Y\n",
    "          X_new = X\n",
    "        elif action_label == \"DOWN\":\n",
    "          Y_new = Y + 1 if Y < (self.grid_dims[0]-1) else Y\n",
    "          X_new = X\n",
    "        elif action_label == \"LEFT\":\n",
    "          Y_new = Y\n",
    "          X_new = X - 1 if X > 0 else X\n",
    "        elif action_label == \"RIGHT\":\n",
    "          Y_new = Y\n",
    "          X_new = X + 1 if X < (self.grid_dims[1]-1) else X\n",
    "        elif action_label == \"STAY\":\n",
    "          Y_new, X_new = Y, X\n",
    "\n",
    "        self.current_location = (Y_new, X_new) # store the new grid location\n",
    "\n",
    "        # Update hunger - resets at food, otherwise increases\n",
    "        if self.current_location == self.food_location:\n",
    "            self.current_hunger = 0\n",
    "        else:\n",
    "            self.current_hunger = min(self.current_hunger + 1, self.max_hunger)\n",
    "\n",
    "        # Update risk - resets at shelter, otherwise increases\n",
    "        if self.current_location == self.shelter_location:\n",
    "            self.current_risk = 0\n",
    "        else:\n",
    "            self.current_risk = min(self.current_risk + 1, self.max_risk)\n",
    "\n",
    "        # Generate observations\n",
    "        loc_obs = self.current_location\n",
    "\n",
    "        hunger_obs = self.hunger_sensor_names[self.hunger_levels.tolist().index(self.current_hunger)]\n",
    "\n",
    "        risk_obs = self.risk_sensor_names[self.risk_levels.tolist().index(self.current_risk)]\n",
    "\n",
    "        return loc_obs, hunger_obs, risk_obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_location = self.init_loc\n",
    "        self.current_hunger = self.init_hunger\n",
    "        self.current_risk = self.init_risk\n",
    "        print(f'Environment Reset: Location to {self.init_loc}, Hunger to {self.init_hunger}, Risk to {self.init_risk}')\n",
    "\n",
    "        # Generate initial observations after reset\n",
    "        loc_obs = self.current_location\n",
    "        hunger_obs = self.hunger_sensor_names[self.hunger_levels.tolist().index(self.current_hunger)]\n",
    "\n",
    "        risk_obs = self.risk_sensor_names[self.risk_levels.tolist().index(self.current_risk)]\n",
    "\n",
    "        return loc_obs, hunger_obs, risk_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5a4edaa4"
   },
   "source": [
    "## Run Active Inference Simulation\n",
    "\n",
    "### Subtask:\n",
    "Initialize the `Agent` with the designed A, B, C, D matrices and the `GridWorldEnv` instance. Run a simulation loop where the agent infers states, infers policies, selects an action, and the environment provides new observations, tracking the agent's path and hunger over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bda7601b"
   },
   "source": [
    "**Reasoning**:\n",
    "The Agent is initialized with:\n",
    "- `B_factor_list` to specify location-dependent transitions for hunger and risk\n",
    "- `control_fac_idx` to specify only the Location factor is controllable\n",
    "\n",
    "The agent then performs active inference: inferring states from observations, evaluating policies based on expected free energy, and selecting actions to balance hunger and risk preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42c8b3a3",
    "outputId": "eaffd796-ec90-4b41-a284-2ce679a362d6"
   },
   "outputs": [],
   "source": [
    "from pymdp.agent import Agent\n",
    "\n",
    "# 1. Instantiate the GridWorldEnv class\n",
    "# Start somewhere interesting - between food and shelter\n",
    "starting_loc = (0, 1)  # Top row, one step from shelter\n",
    "initial_hunger = 0\n",
    "initial_risk = 0\n",
    "\n",
    "my_env = GridWorldEnv(\n",
    "    starting_loc=starting_loc,\n",
    "    initial_hunger=initial_hunger,\n",
    "    initial_risk=initial_risk,\n",
    "    food_location=food_location,\n",
    "    shelter_location=shelter_location,\n",
    "    grid_dims=grid_dims,\n",
    "    max_hunger=max_hunger,\n",
    "    max_risk=max_risk,\n",
    "    loc_list=loc_list,\n",
    "    hunger_levels=hunger_levels,\n",
    "    risk_levels=risk_levels,\n",
    "    hunger_sensor_names=hunger_sensor_names,\n",
    "    risk_sensor_names=risk_sensor_names\n",
    ")\n",
    "\n",
    "# 2. Call the reset() method on the environment instance\n",
    "loc_obs, hunger_obs, risk_obs = my_env.reset()\n",
    "\n",
    "# 3. Convert initial observations from semantic names to numerical indices\n",
    "obs = [\n",
    "    loc_list.index(loc_obs),\n",
    "    hunger_sensor_names.index(hunger_obs),\n",
    "    risk_sensor_names.index(risk_obs)\n",
    "]\n",
    "\n",
    "# 4. Update D to match actual starting state\n",
    "start_loc_idx = loc_list.index(starting_loc)\n",
    "D[0] = utils.onehot(start_loc_idx, num_grid_points)\n",
    "D[1] = utils.onehot(initial_hunger, num_hunger)\n",
    "D[2] = utils.onehot(initial_risk, num_risk)\n",
    "\n",
    "# 5. Initialize the Agent with FACTORIZED model\n",
    "# Key parameters:\n",
    "#   - B_factor_list: specifies which factors each B matrix depends on\n",
    "#   - control_fac_idx: specifies only Location is controllable\n",
    "policy_len = 4\n",
    "\n",
    "my_agent = Agent(\n",
    "    A=A, \n",
    "    B=B, \n",
    "    C=C, \n",
    "    D=D,\n",
    "    # inference_algo=\"MMP\",\n",
    "    # inference_horizon=1,\n",
    "    # use_BMA=False, # try policy_sep_prior=True instead  \n",
    "    # policy_sep_prior=True, # maintain policy-conditioned beliefs  \n",
    "    # sophisticated=True, # Enable pruning  \n",
    "    # si_horizon=5,\n",
    "    # si_policy_prune_threshold=1/32,\n",
    "    # si_state_prune_threshold=1/32, \n",
    "    # si_prune_penalty=128,\n",
    "    policy_len=policy_len,\n",
    "    B_factor_list=B_factor_list,\n",
    "    control_fac_idx=control_fac_idx\n",
    ")\n",
    "\n",
    "# 6. Create empty lists to store the history\n",
    "history_of_locs = [loc_obs]\n",
    "history_of_hunger = [my_env.current_hunger]\n",
    "history_of_risk = [my_env.current_risk]\n",
    "\n",
    "# 7. Simulation parameters\n",
    "T = 20  # number of timesteps\n",
    "\n",
    "print(f\"\\n=== FACTORIZED STATE SPACE SIMULATION ===\")\n",
    "print(f\"State factors: {num_states} (Location × Hunger × Risk)\")\n",
    "print(f\"Total parameters: {sum(num_states)} (vs unified: {np.prod(num_states)})\")\n",
    "print(f\"Starting at {starting_loc}, Hunger={initial_hunger}, Risk={initial_risk}\")\n",
    "print(f\"Food at {food_location} (resets hunger), Shelter at {shelter_location} (resets risk)\")\n",
    "print(f\"Policy length: {policy_len}\")\n",
    "print(f\"B_factor_list: {B_factor_list}\")\n",
    "print(f\"Running {T} timesteps...\")\n",
    "\n",
    "# 8. Start a simulation loop\n",
    "for t in range(T):\n",
    "    print(f\"\\n--- Timestep {t+1}/{T} ---\")\n",
    "    qs = my_agent.infer_states(obs)\n",
    "    my_agent.infer_policies()\n",
    "    chosen_action_id = my_agent.sample_action()\n",
    "    \n",
    "    # For factorized model, chosen_action_id[0] is the movement action\n",
    "    # (hunger and risk factors are uncontrollable, so their \"actions\" are null)\n",
    "    movement_id = int(chosen_action_id[0])\n",
    "    choice_action = actions[movement_id]\n",
    "\n",
    "    print(f'Chosen action: {choice_action}')\n",
    "\n",
    "    loc_obs, hunger_obs, risk_obs = my_env.step(choice_action)\n",
    "\n",
    "    obs = [\n",
    "        loc_list.index(loc_obs),\n",
    "        hunger_sensor_names.index(hunger_obs),\n",
    "        risk_sensor_names.index(risk_obs)\n",
    "    ]\n",
    "\n",
    "    history_of_locs.append(loc_obs)\n",
    "    history_of_hunger.append(my_env.current_hunger)\n",
    "    history_of_risk.append(my_env.current_risk)\n",
    "\n",
    "    print(f'Location: {loc_obs}, Hunger: {my_env.current_hunger}, Risk: {my_env.current_risk}')\n",
    "    if loc_obs == food_location:\n",
    "        print(\"  -> AT FOOD! (hunger reset)\")\n",
    "    if loc_obs == shelter_location:\n",
    "        print(\"  -> AT SHELTER! (risk reset)\")\n",
    "\n",
    "print(\"\\nSimulation finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Agent's Journey\n",
    "\n",
    "Let's visualize the agent's path through the grid, showing the food and shelter locations, and the risk map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "# Convert history of locations to numpy array\n",
    "all_locations = np.vstack(history_of_locs).astype(float)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left plot: Grid with agent path\n",
    "ax = axes[0]\n",
    "\n",
    "# Create grid with pcolormesh for the background\n",
    "X, Y = np.meshgrid(np.arange(grid_dims[1]+1), np.arange(grid_dims[0]+1))\n",
    "ax.pcolormesh(X, Y, np.ones(grid_dims), edgecolors='k', linewidth=2, facecolor='white')\n",
    "\n",
    "# Draw grid squares\n",
    "for y in range(grid_dims[0]):\n",
    "    for x in range(grid_dims[1]):\n",
    "        rect = patches.Rectangle((x, y), 1.0, 1.0, linewidth=2, \n",
    "                                  edgecolor='k', facecolor='white')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Mark food location (green)\n",
    "food_rect = patches.Rectangle((food_location[1], food_location[0]), 1.0, 1.0, \n",
    "                               linewidth=5, edgecolor='green', facecolor='lightgreen')\n",
    "ax.add_patch(food_rect)\n",
    "ax.text(food_location[1]+0.5, food_location[0]+0.5, 'Food\\n(resets hunger)', fontsize=9, \n",
    "        ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Mark shelter location (blue)  \n",
    "shelter_rect = patches.Rectangle((shelter_location[1], shelter_location[0]), 1.0, 1.0,\n",
    "                                  linewidth=5, edgecolor='blue', facecolor='lightblue')\n",
    "ax.add_patch(shelter_rect)\n",
    "ax.text(shelter_location[1]+0.5, shelter_location[0]+0.5, 'Shelter\\n(resets risk)', fontsize=9,\n",
    "        ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Plot the agent's path\n",
    "ax.plot(all_locations[:,1]+0.5, all_locations[:,0]+0.5, 'darkred', linewidth=2, zorder=2, alpha=0.7)\n",
    "\n",
    "# Color the dots by time\n",
    "temporal_colormap = cm.hot(np.linspace(0, 0.8, len(all_locations)))\n",
    "dots = ax.scatter(all_locations[:,1]+0.5, all_locations[:,0]+0.5, 200, \n",
    "                  c=temporal_colormap, zorder=3, edgecolors='black', linewidth=1)\n",
    "\n",
    "# Mark start and end positions\n",
    "ax.scatter(all_locations[0,1]+0.5, all_locations[0,0]+0.5, 300, c='lime', \n",
    "           marker='s', zorder=4, edgecolors='black', linewidth=2, label='Start')\n",
    "ax.scatter(all_locations[-1,1]+0.5, all_locations[-1,0]+0.5, 300, c='purple',\n",
    "           marker='*', zorder=4, edgecolors='black', linewidth=2, label='End')\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap='hot', norm=plt.Normalize(vmin=0, vmax=T))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax, shrink=0.6)\n",
    "cbar.set_label('Timestep', fontsize=12)\n",
    "\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.set_xlim(0, grid_dims[1])\n",
    "ax.set_ylim(grid_dims[0], 0)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('X', fontsize=12)\n",
    "ax.set_ylabel('Y', fontsize=12)\n",
    "ax.set_title(f\"Agent's Journey in Grid World\", fontsize=14)\n",
    "\n",
    "# Right plot: Hunger and Risk over time\n",
    "ax2 = axes[1]\n",
    "timesteps = np.arange(len(history_of_hunger))\n",
    "ax2.plot(timesteps, history_of_hunger, 'g-o', label='Hunger', linewidth=2, markersize=6)\n",
    "ax2.plot(timesteps, history_of_risk, 'r-s', label='Risk', linewidth=2, markersize=6)\n",
    "ax2.axhline(y=max_hunger, color='g', linestyle='--', alpha=0.5, label=f'Max Hunger ({max_hunger})')\n",
    "ax2.axhline(y=max_risk, color='r', linestyle='--', alpha=0.5, label=f'Max Risk ({max_risk})')\n",
    "ax2.set_xlabel('Timestep', fontsize=12)\n",
    "ax2.set_ylabel('Level', fontsize=12)\n",
    "ax2.set_title('Hunger and Risk Levels Over Time', fontsize=14)\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylim(-0.5, max(max_hunger, max_risk) + 1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nJourney Summary:\")\n",
    "print(f\"  Start: {history_of_locs[0]}, Hunger: {history_of_hunger[0]}, Risk: {history_of_risk[0]}\")\n",
    "print(f\"  End: {history_of_locs[-1]}, Hunger: {history_of_hunger[-1]}, Risk: {history_of_risk[-1]}\")\n",
    "print(f\"  Total steps: {len(history_of_locs)-1}\")\n",
    "print(f\"  Times at food: {sum(1 for loc in history_of_locs if loc == food_location)}\")\n",
    "print(f\"  Times at shelter: {sum(1 for loc in history_of_locs if loc == shelter_location)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Probabilistic Distance-Dependent Drives\n",
    "\n",
    "In this experiment, we modify the environment and the agent's generative model to test probabilistic reward dynamics based on distance.\n",
    "\n",
    "This creates a longer \"scent trail\" that the agent can detect from the center of the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Experiment 2: Distance-Dependent Penalties ===\n",
    "\n",
    "# 1. Start with same grid dimensions and max levels\n",
    "# New locations: Opposite corners\n",
    "food_location2 = (4, 4)      # Bottom-right\n",
    "shelter_location2 = (0, 0)   # Top-left (unchanged)\n",
    "\n",
    "# Pre-compute indices for new locations\n",
    "food_idx2 = loc_list.index(food_location2)\n",
    "shelter_idx2 = loc_list.index(shelter_location2)\n",
    "\n",
    "print(\"Experiment 2 Setup:\")\n",
    "print(f\"  Food: {food_location2}, Shelter: {shelter_location2}\")\n",
    "\n",
    "# 2. Define Custom Environment with \"Spread Out\" Penalties\n",
    "class GridWorldEnvDistancePenalty(GridWorldEnv):\n",
    "    def __init__(self, starting_loc, initial_hunger, initial_risk, food_location, shelter_location, \n",
    "                 grid_dims, max_hunger, max_risk, loc_list, hunger_levels, risk_levels,\n",
    "                 hunger_sensor_names, risk_sensor_names):\n",
    "        self.init_loc = starting_loc\n",
    "        self.init_hunger = initial_hunger\n",
    "        self.init_risk = initial_risk\n",
    "        self.current_location = self.init_loc\n",
    "        self.current_hunger = self.init_hunger\n",
    "        self.current_risk = self.init_risk\n",
    "\n",
    "        self.food_location = food_location\n",
    "        self.shelter_location = shelter_location\n",
    "        self.grid_dims = grid_dims\n",
    "        self.max_hunger = max_hunger\n",
    "        self.max_risk = max_risk\n",
    "        self.loc_list = loc_list\n",
    "        self.hunger_levels = hunger_levels\n",
    "        self.risk_levels = risk_levels\n",
    "        self.hunger_sensor_names = hunger_sensor_names\n",
    "        self.risk_sensor_names = risk_sensor_names\n",
    "\n",
    "        print(f'Environment Initialized: Starting location is {self.init_loc}, Initial hunger is {self.init_hunger}, Initial risk is {self.init_risk}')\n",
    "\n",
    "    def step(self, action_label):\n",
    "        Y, X = self.current_location\n",
    "\n",
    "        if action_label == \"UP\":\n",
    "          Y_new = Y - 1 if Y > 0 else Y\n",
    "          X_new = X\n",
    "        elif action_label == \"DOWN\":\n",
    "          Y_new = Y + 1 if Y < (self.grid_dims[0]-1) else Y\n",
    "          X_new = X\n",
    "        elif action_label == \"LEFT\":\n",
    "          Y_new = Y\n",
    "          X_new = X - 1 if X > 0 else X\n",
    "        elif action_label == \"RIGHT\":\n",
    "          Y_new = Y\n",
    "          X_new = X + 1 if X < (self.grid_dims[1]-1) else X\n",
    "        elif action_label == \"STAY\":\n",
    "          Y_new, X_new = Y, X\n",
    "\n",
    "        self.current_location = (Y_new, X_new)\n",
    "\n",
    "        # Distance calculations\n",
    "        dist_food = abs(self.current_location[0]-self.food_location[0]) + abs(self.current_location[1]-self.food_location[1])\n",
    "        dist_shelter = abs(self.current_location[0]-self.shelter_location[0]) + abs(self.current_location[1]-self.shelter_location[1])\n",
    "\n",
    "        # New Hunger Logic\n",
    "        # Linear gradient to ensure \"scent\" is visible everywhere\n",
    "        p_h = max(0.0, 1.0 - (dist_food * 0.15))\n",
    "        \n",
    "        if np.random.rand() < p_h:\n",
    "            dh = -1\n",
    "        else:\n",
    "            dh = 1 # Natural increase if reward missed\n",
    "            \n",
    "        self.current_hunger = int(np.clip(self.current_hunger + dh, 0, self.max_hunger))\n",
    "\n",
    "        # New Risk Logic\n",
    "        p_r = max(0.0, 1.0 - (dist_shelter * 0.15))\n",
    "            \n",
    "        if np.random.rand() < p_r:\n",
    "            dr = -1\n",
    "        else:\n",
    "            dr = 1\n",
    "            \n",
    "        self.current_risk = int(np.clip(self.current_risk + dr, 0, self.max_risk))\n",
    "\n",
    "        # Observations\n",
    "        loc_obs = self.current_location\n",
    "        # Map values to closest available level if needed, but here levels are just integers\n",
    "        hunger_obs = self.hunger_sensor_names[self.hunger_levels.tolist().index(self.current_hunger)]\n",
    "        risk_obs = self.risk_sensor_names[self.risk_levels.tolist().index(self.current_risk)]\n",
    "\n",
    "        return loc_obs, hunger_obs, risk_obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_location = self.init_loc\n",
    "        self.current_hunger = self.init_hunger\n",
    "        self.current_risk = self.init_risk\n",
    "        print(f'Environment Reset: Location to {self.init_loc}, Hunger to {self.init_hunger}, Risk to {self.init_risk}')\n",
    "\n",
    "        # Generate initial observations after reset\n",
    "        loc_obs = self.current_location\n",
    "        hunger_obs = self.hunger_sensor_names[self.hunger_levels.tolist().index(self.current_hunger)]\n",
    "\n",
    "        risk_obs = self.risk_sensor_names[self.risk_levels.tolist().index(self.current_risk)]\n",
    "\n",
    "        return loc_obs, hunger_obs, risk_obs\n",
    "    \n",
    "\n",
    "# 3. New B Matrices with Distance-Based Transitions\n",
    "B2 = utils.obj_array(len(num_states))\n",
    "\n",
    "# B[0] (Location) - Same as before (Standard movement)\n",
    "B2[0] = B[0].copy()\n",
    "\n",
    "# B[1] (Hunger) - Distance dependent\n",
    "B2[1] = np.zeros((num_hunger, num_grid_points, num_hunger, num_controls[1]))\n",
    "print(\"Building B2[1] (Hunger)...\")\n",
    "\n",
    "for loc_idx in range(num_grid_points):\n",
    "    curr_y, curr_x = loc_list[loc_idx]\n",
    "    dist = abs(curr_y - food_location2[0]) + abs(curr_x - food_location2[1])\n",
    "    \n",
    "    # Linear gradient matching environment\n",
    "    p_h = max(0.0, 1.0 - (dist * 0.15))\n",
    "        \n",
    "    for h in range(num_hunger):\n",
    "        # Two possible outcomes:\n",
    "        # 1. Reward (-1) with prob p_h\n",
    "        # 2. No reward (+1) with prob 1-p_h\n",
    "        \n",
    "        next_h_dec = max(0, h - 1)\n",
    "        next_h_inc = min(max_hunger, h + 1)\n",
    "        \n",
    "        if next_h_dec == next_h_inc: # Edge case if max_hunger=0? unlikely\n",
    "             B2[1][next_h_dec, loc_idx, h, 0] = 1.0\n",
    "        else:\n",
    "            B2[1][next_h_dec, loc_idx, h, 0] += p_h\n",
    "            B2[1][next_h_inc, loc_idx, h, 0] += (1.0 - p_h)\n",
    "\n",
    "# B[2] (Risk) - Distance dependent\n",
    "B2[2] = np.zeros((num_risk, num_grid_points, num_risk, num_controls[2]))\n",
    "print(\"Building B2[2] (Risk)...\")\n",
    "\n",
    "for loc_idx in range(num_grid_points):\n",
    "    curr_y, curr_x = loc_list[loc_idx]\n",
    "    dist = abs(curr_y - shelter_location2[0]) + abs(curr_x - shelter_location2[1])\n",
    "    \n",
    "    # Linear gradient matching environment\n",
    "    p_r = max(0.0, 1.0 - (dist * 0.15))\n",
    "        \n",
    "    for r in range(num_risk):\n",
    "        next_r_dec = max(0, r - 1)\n",
    "        next_r_inc = min(max_risk, r + 1)\n",
    "        \n",
    "        if next_r_dec == next_r_inc:\n",
    "             B2[2][next_r_dec, loc_idx, r, 0] = 1.0\n",
    "        else:\n",
    "            B2[2][next_r_dec, loc_idx, r, 0] += p_r\n",
    "            B2[2][next_r_inc, loc_idx, r, 0] += (1.0 - p_r)\n",
    "\n",
    "# Verify B2\n",
    "for i, b_fac in enumerate(B2):\n",
    "    if not np.allclose(b_fac.sum(axis=0), 1.0):\n",
    "        print(f\"Warning: B2[{i}] not normalized!\")\n",
    "\n",
    "# 4. New A Matrices (Need to update for new food/shelter locations)\n",
    "# A[0] is identity (same)\n",
    "# A[1] and A[2] depend on specific locations\n",
    "A2 = utils.obj_array(len(num_obs))\n",
    "A2[0] = A[0].copy()\n",
    "\n",
    "# A[1] Hunger\n",
    "A2[1] = np.zeros((num_obs[1], num_grid_points, num_hunger, num_risk)) # Shapes same as before\n",
    "for loc_idx in range(num_grid_points):\n",
    "    for h in range(num_hunger):\n",
    "        for r in range(num_risk):\n",
    "            if loc_idx == food_idx2:\n",
    "                A2[1][0, loc_idx, h, r] = 1.0 # Satiated at food\n",
    "            else:\n",
    "                A2[1][h, loc_idx, h, r] = 1.0 # True hunger elsewhere\n",
    "\n",
    "# A[2] Risk\n",
    "A2[2] = np.zeros((num_obs[2], num_grid_points, num_hunger, num_risk))\n",
    "for loc_idx in range(num_grid_points):\n",
    "    for h in range(num_hunger):\n",
    "        for r in range(num_risk):\n",
    "            if loc_idx == shelter_idx2:\n",
    "                A2[2][0, loc_idx, h, r] = 1.0 # Safe at shelter\n",
    "            else:\n",
    "                A2[2][r, loc_idx, h, r] = 1.0 # True risk elsewhere\n",
    "\n",
    "# 5. Run Simulation\n",
    "initial_hunger2 = 3\n",
    "initial_risk2 = 3\n",
    "starting_loc2 = (2, 2) # Start in middle\n",
    "\n",
    "my_env2 = GridWorldEnvDistancePenalty(\n",
    "    starting_loc=starting_loc2,\n",
    "    initial_hunger=initial_hunger2,\n",
    "    initial_risk=initial_risk2,\n",
    "    food_location=food_location2,\n",
    "    shelter_location=shelter_location2,\n",
    "    grid_dims=grid_dims,\n",
    "    max_hunger=max_hunger,\n",
    "    max_risk=max_risk,\n",
    "    loc_list=loc_list,\n",
    "    hunger_levels=hunger_levels,\n",
    "    risk_levels=risk_levels,\n",
    "    hunger_sensor_names=hunger_sensor_names,\n",
    "    risk_sensor_names=risk_sensor_names\n",
    ")\n",
    "\n",
    "# Update Initial Belief D\n",
    "D2 = utils.obj_array_uniform(num_states)\n",
    "D2[0] = utils.onehot(loc_list.index(starting_loc2), num_grid_points)\n",
    "D2[1] = utils.onehot(initial_hunger2, num_hunger)\n",
    "D2[2] = utils.onehot(initial_risk2, num_risk)\n",
    "\n",
    "# Tweak C to encourage leaving sooner (via EXPONENTIAL risk/hunger avoidance)\n",
    "# KEY FIX: Remove positive rewards. \"Satiated\" and \"Safe\" should just be neutral (0.0).\n",
    "# Having +10.0 reward for doing nothing (staying at food) makes it hard to leave.\n",
    "# Also lower steepness so early risk levels (1, 2) are felt more strongly.\n",
    "C[1][0] = 0.0 \n",
    "C[2][0] = 0.0\n",
    "\n",
    "penalty_approach2 = -30.0 # Stronger penalty\n",
    "exp_steepness = 1.5      # Lower steepness = earlier pain (High steepness was hiding low risk)\n",
    "\n",
    "# Risk Preferences (Factor 2)\n",
    "if n_risk > 2:\n",
    "    for i in range(1, n_risk - 1):\n",
    "        denominator = (n_risk - 2) - 1\n",
    "        progress = (i - 1) / denominator if denominator > 0 else 1.0\n",
    "        \n",
    "        # Exponential scaling: (e^(kx) - 1) / (e^k - 1)\n",
    "        # Result: Low penalty for low risk, massive penalty for high risk\n",
    "        scale = (np.exp(exp_steepness * progress) - 1) / (np.exp(exp_steepness) - 1)\n",
    "        C[2][i] = penalty_approach2 * scale\n",
    "\n",
    "# Hunger Preferences (Factor 1) - symmetrical\n",
    "if n_hunger > 2:\n",
    "    for i in range(1, n_hunger - 1):\n",
    "        denominator = (n_hunger - 2) - 1\n",
    "        progress = (i - 1) / denominator if denominator > 0 else 1.0\n",
    "        \n",
    "        # Exponential scaling\n",
    "        scale = (np.exp(exp_steepness * progress) - 1) / (np.exp(exp_steepness) - 1)\n",
    "        C[1][i] = penalty_approach2 * scale\n",
    "\n",
    "# Agent\n",
    "policy_len2 = 2\n",
    "my_agent2 = Agent(A=A2, B=B2, C=C, D=D2, policy_len=policy_len2, \n",
    "                  B_factor_list=B_factor_list, control_fac_idx=control_fac_idx)\n",
    "\n",
    "# Loop\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "T2 = 100 # Increased duration to see oscillation\n",
    "history_of_locs2 = [starting_loc2]\n",
    "history_of_hunger2 = [initial_hunger2]\n",
    "history_of_risk2 = [initial_risk2]\n",
    "obs2 = my_env2.reset() \n",
    "obs2 = [loc_list.index(obs2[0]), hunger_sensor_names.index(obs2[1]), risk_sensor_names.index(obs2[2])]\n",
    "\n",
    "# Pre-calculate Heatmaps for visualization efficiency\n",
    "hunger_grid = np.zeros(grid_dims)\n",
    "risk_grid = np.zeros(grid_dims)\n",
    "for y in range(grid_dims[0]):\n",
    "    for x in range(grid_dims[1]):\n",
    "         d_f = abs(y - food_location2[0]) + abs(x - food_location2[1])\n",
    "         hunger_grid[y, x] = max(0.0, 1.0 - (d_f * 0.15))\n",
    "         d_s = abs(y - shelter_location2[0]) + abs(x - shelter_location2[1])\n",
    "         risk_grid[y, x] = max(0.0, 1.0 - (d_s * 0.15))\n",
    "\n",
    "print(\"\\nStarting Experiment 2 Simulation...\")\n",
    "\n",
    "for t in range(T2):\n",
    "    # --- Agent Step ---\n",
    "    qs = my_agent2.infer_states(obs2)\n",
    "    my_agent2.infer_policies()\n",
    "    action_idx = int(my_agent2.sample_action()[0])\n",
    "    action = actions[action_idx]\n",
    "    \n",
    "    loc_obs, hunger_obs, risk_obs = my_env2.step(action)\n",
    "    \n",
    "    obs2 = [loc_list.index(loc_obs), hunger_sensor_names.index(hunger_obs), risk_sensor_names.index(risk_obs)]\n",
    "    \n",
    "    history_of_locs2.append(loc_obs)\n",
    "    history_of_hunger2.append(my_env2.current_hunger)\n",
    "    history_of_risk2.append(my_env2.current_risk)\n",
    "\n",
    "    # --- Live Visualization ---\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Agent Path (Top Left)\n",
    "    ax_path = axes[0, 0]\n",
    "    ax_path.set_title(f\"Step {t+1}/{T2}: Agent Path\")\n",
    "    ax_path.invert_yaxis()\n",
    "    \n",
    "    # Grid Background\n",
    "    for y in range(grid_dims[0]):\n",
    "        for x in range(grid_dims[1]):\n",
    "            rect = patches.Rectangle((x, y), 1.0, 1.0, linewidth=1, edgecolor='lightgray', facecolor='white')\n",
    "            ax_path.add_patch(rect)\n",
    "    \n",
    "    # Trajectory & Current Position\n",
    "    hist_locs = np.vstack(history_of_locs2).astype(float)\n",
    "    temporal_colormap = cm.hot(np.linspace(0, 0.8, len(hist_locs)))\n",
    "    \n",
    "    # Path line\n",
    "    if len(hist_locs) > 1:\n",
    "        ax_path.plot(hist_locs[:,1]+0.5, hist_locs[:,0]+0.5, 'darkred', linewidth=2, zorder=2, alpha=0.3)\n",
    "    \n",
    "    # Points\n",
    "    ax_path.scatter(hist_locs[:,1]+0.5, hist_locs[:,0]+0.5, 200, c=temporal_colormap, zorder=3, edgecolors='black')\n",
    "    \n",
    "    # Current Head\n",
    "    current_head = hist_locs[-1]\n",
    "    ax_path.scatter(current_head[1]+0.5, current_head[0]+0.5, 300, c='yellow', marker='*', zorder=4, edgecolors='black', label='Current')\n",
    "\n",
    "    # Markers\n",
    "    ax_path.text(food_location2[1]+0.5, food_location2[0]+0.5, 'F', ha='center', va='center', fontweight='bold', color='forestgreen', fontsize=14)\n",
    "    ax_path.text(shelter_location2[1]+0.5, shelter_location2[0]+0.5, 'S', ha='center', va='center', fontweight='bold', color='royalblue', fontsize=14)\n",
    "\n",
    "    # 2. Stats (Top Right)\n",
    "    ax_stats = axes[0, 1]\n",
    "    ax_stats.set_title(\"Internal Drives\")\n",
    "    ax_stats.plot(history_of_hunger2, 'g-o', label='Hunger')\n",
    "    ax_stats.plot(history_of_risk2, 'r-s', label='Risk')\n",
    "    ax_stats.axhline(y=max_hunger, color='k', linestyle='--', alpha=0.3)\n",
    "    ax_stats.set_xlim(0, T2)\n",
    "    ax_stats.set_ylim(-0.5, max(max_hunger, max_risk)+1)\n",
    "    ax_stats.legend(loc='upper left')\n",
    "    ax_stats.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Hunger Map (Bottom Left)\n",
    "    ax_map1 = axes[1, 0]\n",
    "    ax_map1.set_title(\"Hunger Relief Probability\")\n",
    "    im1 = ax_map1.imshow(hunger_grid, cmap='Greens', vmin=0, vmax=1.0)\n",
    "    fig.colorbar(im1, ax=ax_map1, shrink=0.7)\n",
    "    \n",
    "    # 4. Risk Map (Bottom Right)\n",
    "    ax_map2 = axes[1, 1]\n",
    "    ax_map2.set_title(\"Risk Relief Probability\")\n",
    "    im2 = ax_map2.imshow(risk_grid, cmap='Blues', vmin=0, vmax=1.0)\n",
    "    fig.colorbar(im2, ax=ax_map2, shrink=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Experiment 2 Finished.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
